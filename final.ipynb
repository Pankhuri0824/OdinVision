{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745256370.392043   10779 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1745256370.395323   10840 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.1.3-1pop0~1689084530~22.04~0618746), renderer: Mesa Intel(R) Xe Graphics (TGL GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1745256370.417309   10824 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745256370.435347   10829 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745256375.230549   10827 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.008019621271004697\n",
      "0.002186282361398815\n",
      "0.0024432569282657737\n",
      "0.05174710470288737\n",
      "0.053421647475329716\n",
      "0.02575013901794636\n",
      "0.0024432569282657737\n",
      "0.024672862179861053\n",
      "0.0013238257112571667\n",
      "0.002186282361398815\n",
      "0.0024432569282657737\n",
      "0.0024432569282657737\n",
      "0.002186282361398815\n",
      "0.0024432569282657737\n",
      "0.0024432569282657737\n",
      "0.0019462462364009702\n",
      "0.0019462462364009702\n",
      "0.05010445722599638\n",
      "0.03280446357795494\n",
      "0.0024432569282657737\n",
      "0.0030095361932196726\n",
      "0.002186282361398815\n",
      "0.002186282361398815\n",
      "0.015372788152685392\n",
      "0.0\n",
      "0.0\n",
      "0.0453658286662024\n",
      "0.04691402634486509\n",
      "0.05010445722599638\n",
      "0.04849349992525135\n",
      "0.04849349992525135\n",
      "0.04849349992525135\n",
      "0.04691402634486509\n",
      "0.043848697680428345\n",
      "0.04090679183056204\n",
      "0.0367216198751518\n",
      "0.03408076916670969\n",
      "0.03280446357795494\n",
      "0.03155727423341512\n",
      "0.060442828547762714\n",
      "0.07196553619175161\n",
      "0.09630515818302952\n",
      "0.10622650606220717\n",
      "0.10879847213864237\n",
      "0.10879847213864237\n",
      "0.14566478332868943\n",
      "0.1615891987632973\n",
      "0.18915933913537358\n",
      "0.20393133869836993\n",
      "0.21545038025175411\n",
      "0.2354993119357962\n",
      "0.2609848033590808\n",
      "0.29272582913745554\n",
      "0.42446900774933405\n",
      "0.45439328990753747\n",
      "0.48554889156941183\n",
      "0.48554889156941183\n",
      "0.4667068466349758\n",
      "0.4605253754128831\n",
      "0.37910962160739675\n",
      "0.41284033935058456\n",
      "0.448310457894244\n",
      "0.5048401344661522\n",
      "0.531266613819912\n",
      "0.5379999557417107\n",
      "0.5516196043859122\n",
      "0.5724334022399462\n",
      "0.5724334022399462\n",
      "0.5724334022399462\n",
      "0.586566964507944\n",
      "0.586566964507944\n",
      "0.6451845846719569\n",
      "0.586566964507944\n",
      "0.652748223756082\n",
      "0.6228101578364865\n",
      "0.630215675438684\n",
      "0.630215675438684\n",
      "0.6228101578364865\n",
      "0.6009078622267247\n",
      "0.586566964507944\n",
      "0.6081563758376733\n",
      "0.6009078622267247\n",
      "0.6009078622267247\n",
      "0.593711433568583\n",
      "0.586566964507944\n",
      "0.4983595918022738\n",
      "0.4605253754128831\n",
      "0.42446900774933405\n",
      "0.3167482608401139\n",
      "0.2566278806721503\n",
      "0.3021984735357028\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import threading\n",
    "\n",
    "# Load grayscale depth map\n",
    "depth_map_original = cv2.imread(\"test_depth.png\", cv2.IMREAD_GRAYSCALE)\n",
    "if depth_map_original is None:\n",
    "    print(\"‚ùå Error: Could not load the png. Check the file path.\")\n",
    "    exit()\n",
    "\n",
    "# Load translucent image for camera overlay\n",
    "camera_overlay = cv2.imread(\"test_og.png\", cv2.IMREAD_UNCHANGED)\n",
    "if camera_overlay is None:\n",
    "    print(\"‚ùå Error: Camera overlay missing.\")\n",
    "    camera_overlay = np.zeros_like(depth_map_original)\n",
    "\n",
    "# Set overlay transparency\n",
    "overlay_opacity = 0.4  \n",
    "\n",
    "# Sound settings\n",
    "bass_freq = 50  \n",
    "sample_rate = 44100  \n",
    "volume = 0.10  \n",
    "fade_speed = 0.12  \n",
    "texture_variability = 0.5  \n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils  \n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "# Start webcam capture\n",
    "# cap = cv2.VideoCapture(0)\n",
    "cap = cv2.VideoCapture('/dev/video2')\n",
    "if not cap.isOpened():\n",
    "    print(\"‚ùå Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Get camera frame size\n",
    "cam_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "cam_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Function to resize depth map while maintaining aspect ratio\n",
    "def resize_depth_map(depth_map, target_width, target_height):\n",
    "    h, w = depth_map.shape[:2]\n",
    "    scale = min(target_width / w, target_height / h)  # Maintain aspect ratio\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "\n",
    "    resized_depth_map = cv2.resize(depth_map, (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Create a blank canvas of target size\n",
    "    canvas = np.zeros((target_height, target_width), dtype=np.uint8)\n",
    "    y_offset, x_offset = (target_height - new_h) // 2, (target_width - new_w) // 2\n",
    "    canvas[y_offset:y_offset + new_h, x_offset:x_offset + new_w] = resized_depth_map\n",
    "\n",
    "    return canvas\n",
    "\n",
    "depth_map = resize_depth_map(depth_map_original, cam_width, cam_height)\n",
    "\n",
    "# Function to get vibration intensity from depth\n",
    "def get_vibration_intensity(x, y):\n",
    "    if 0 <= x < cam_width and 0 <= y < cam_height:\n",
    "        depth_value = depth_map[y, x] / 255.0  \n",
    "        return depth_value ** 2.5  \n",
    "    return 0  \n",
    "\n",
    "# Function to generate smooth sound transitions\n",
    "def smooth_transition():\n",
    "    global current_intensity, target_intensity, running\n",
    "    while running:\n",
    "        current_intensity += (target_intensity - current_intensity) * fade_speed\n",
    "        sd.sleep(10)\n",
    "\n",
    "# Start smooth intensity transition thread\n",
    "running = True\n",
    "current_intensity = 0\n",
    "target_intensity = 0\n",
    "smoothing_thread = threading.Thread(target=smooth_transition, daemon=True)\n",
    "smoothing_thread.start()\n",
    "\n",
    "# Function to generate bass-heavy sound wave\n",
    "def generate_waveform(intensity, frames):\n",
    "    t = np.linspace(0, frames / sample_rate, frames, endpoint=False)\n",
    "    sine_wave = np.sin(2 * np.pi * bass_freq * t)\n",
    "    noise = np.random.uniform(-1, 1, frames) * 0.02 * texture_variability\n",
    "    return (sine_wave + noise) * intensity * volume\n",
    "\n",
    "# Audio callback function\n",
    "def audio_callback(outdata, frames, time, status):\n",
    "    global current_intensity\n",
    "    try:\n",
    "        if status:\n",
    "            print(\"‚ö†Ô∏è Audio Stream Warning:\", status)\n",
    "        wave = generate_waveform(current_intensity, frames) if current_intensity > 0 else np.zeros(frames)\n",
    "        outdata[:] = np.column_stack((wave, wave))  \n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Audio error:\", e)\n",
    "\n",
    "# Start audio stream safely\n",
    "try:\n",
    "    stream = sd.OutputStream(callback=audio_callback, samplerate=sample_rate, channels=2)\n",
    "    stream.start()\n",
    "except Exception as e:\n",
    "    print(\"‚ùå SoundDevice Error:\", e)\n",
    "    stream = None  \n",
    "\n",
    "# Function to resize overlay while maintaining aspect ratio\n",
    "def resize_overlay(overlay, target_width, target_height):\n",
    "    h, w = overlay.shape[:2]\n",
    "    scale = min(target_width / w, target_height / h)  \n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "\n",
    "    resized_overlay = cv2.resize(overlay, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    if resized_overlay.shape[2] == 3:\n",
    "        alpha_channel = np.ones((new_h, new_w), dtype=np.uint8) * 255  \n",
    "        resized_overlay = np.dstack((resized_overlay, alpha_channel))\n",
    "\n",
    "    canvas = np.zeros((target_height, target_width, 4), dtype=np.uint8)  \n",
    "    y_offset, x_offset = (target_height - new_h) // 2, (target_width - new_w) // 2\n",
    "    canvas[y_offset:y_offset + new_h, x_offset:x_offset + new_w] = resized_overlay\n",
    "    return canvas\n",
    "\n",
    "cached_overlay = resize_overlay(camera_overlay, cam_width, cam_height)\n",
    "\n",
    "# Main loop: track finger & control sound\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    depth_map_display = cv2.cvtColor(depth_map, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            index_finger_tip = hand_landmarks.landmark[8]\n",
    "            x = int(index_finger_tip.x * cam_width)\n",
    "            y = int(index_finger_tip.y * cam_height)\n",
    "            x_pos, y_pos = np.clip(x, 0, cam_width - 1), np.clip(y, 0, cam_height - 1)\n",
    "            target_intensity = get_vibration_intensity(x_pos, y_pos)\n",
    "            print(target_intensity)\n",
    "            cv2.circle(depth_map_display, (x_pos, y_pos), 8, (0, 255, 0), -1)\n",
    "            cv2.circle(frame, (x_pos, y_pos), int(15 + target_intensity * 40), (255, 255, 255), max(1, int(2 + target_intensity * 4)))\n",
    "    else:\n",
    "        target_intensity = 0  \n",
    "\n",
    "    alpha = cached_overlay[:, :, 3] / 255.0\n",
    "    for c in range(3):\n",
    "        frame[:, :, c] = (1 - overlay_opacity * alpha) * frame[:, :, c] + (overlay_opacity * alpha) * cached_overlay[:, :, c]\n",
    "\n",
    "    cv2.imshow(\"Camera Feed with Hand Detection\", frame)\n",
    "    cv2.imshow(\"Depth Map\", depth_map_display)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "running = False\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "if stream:\n",
    "    stream.stop()\n",
    "    stream.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved as color_classifier.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv(\"color_training_set.csv\")\n",
    "\n",
    "# Extract features (RGB values) and labels (Color names)\n",
    "X = df[['R', 'G', 'B']].values  # Features\n",
    "y = df['Label'].values  # Labels\n",
    "\n",
    "# Train a K-Nearest Neighbors model\n",
    "knn = KNeighborsClassifier(n_neighbors=3)  # Use n_neighbors=3 for better accuracy\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Save the model for later use\n",
    "with open(\"color_classifier.pkl\", \"wb\") as f:\n",
    "    pickle.dump(knn, f)\n",
    "\n",
    "print(\"‚úÖ Model saved as color_classifier.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singhasaur/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SHAP_R</th>\n",
       "      <th>SHAP_G</th>\n",
       "      <th>SHAP_B</th>\n",
       "      <th>R</th>\n",
       "      <th>G</th>\n",
       "      <th>B</th>\n",
       "      <th>Predicted_Label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.076667</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>4</td>\n",
       "      <td>222</td>\n",
       "      <td>10</td>\n",
       "      <td>Green</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.082778</td>\n",
       "      <td>-0.293889</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>234</td>\n",
       "      <td>Blue</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.270000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>223</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>Red</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.070556</td>\n",
       "      <td>-0.292778</td>\n",
       "      <td>0.122222</td>\n",
       "      <td>48</td>\n",
       "      <td>243</td>\n",
       "      <td>32</td>\n",
       "      <td>Green</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.270000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>227</td>\n",
       "      <td>34</td>\n",
       "      <td>39</td>\n",
       "      <td>Red</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     SHAP_R    SHAP_G    SHAP_B    R    G    B Predicted_Label  Class\n",
       "0  0.076667 -0.293333  0.116667    4  222   10           Green      0\n",
       "1  0.111111  0.082778 -0.293889   19   17  234            Blue      0\n",
       "2 -0.270000  0.050000  0.120000  223   35    1             Red      0\n",
       "3  0.070556 -0.292778  0.122222   48  243   32           Green      0\n",
       "4 -0.270000  0.050000  0.120000  227   34   39             Red      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shap\n",
    "import pandas as pd\n",
    "\n",
    "explainer = shap.Explainer(knn.predict_proba, X)\n",
    "shap_values = explainer(X)\n",
    "shap_dfs = []\n",
    "\n",
    "for class_idx in range(shap_values.values.shape[2]):\n",
    "    shap_class_values = shap_values.values[:, :, class_idx]\n",
    "    shap_df = pd.DataFrame(shap_class_values, columns=['SHAP_R', 'SHAP_G', 'SHAP_B'])\n",
    "    \n",
    "    shap_df['R'] = X[:, 0]\n",
    "    shap_df['G'] = X[:, 1]\n",
    "    shap_df['B'] = X[:, 2]\n",
    "\n",
    "    shap_df['Predicted_Label'] = knn.predict(X)\n",
    "\n",
    "    shap_df['Class'] = class_idx\n",
    "    \n",
    "    shap_dfs.append(shap_df)\n",
    "\n",
    "# Concatenate all DataFrames for each class into one final DataFrame\n",
    "final_shap_df = pd.concat(shap_dfs, ignore_index=True)\n",
    "\n",
    "final_shap_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the DataFrame to a format that SHAP understands\n",
    "shap_values_for_plot = shap.Explanation(\n",
    "    values=final_shap_df[['SHAP_R', 'SHAP_G', 'SHAP_B']].values, \n",
    "    data=final_shap_df[['R', 'G', 'B']].values,\n",
    "    feature_names=['R', 'G', 'B']\n",
    ")\n",
    "\n",
    "# Create a summary plot\n",
    "shap.summary_plot(shap_values_for_plot, final_shap_df[['R', 'G', 'B']].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load trained KNN model\n",
    "with open(\"color_classifier.pkl\", \"rb\") as f:\n",
    "    knn = pickle.load(f)\n",
    "\n",
    "# Load dataset to dynamically create color map\n",
    "df = pd.read_csv(\"color_training_set.csv\")\n",
    "color_map = {row[\"Label\"]: [row[\"R\"], row[\"G\"], row[\"B\"]] for _, row in df.iterrows()}\n",
    "\n",
    "# Load the target image\n",
    "image = cv2.imread(\"test_og.png\")\n",
    "if image is None:\n",
    "    print(\"‚ùå Error: Could not load image.\")\n",
    "    exit()\n",
    "\n",
    "# Convert BGR (OpenCV format) to RGB\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Reshape the image into a 2D array of pixels\n",
    "pixels = image_rgb.reshape(-1, 3)\n",
    "\n",
    "# Predict colors for each pixel\n",
    "predicted_labels = knn.predict(pixels)\n",
    "\n",
    "# Convert predicted labels into an image using the dynamic color map\n",
    "classified_pixels = np.array([color_map[label] for label in predicted_labels], dtype=np.uint8)\n",
    "\n",
    "# Reshape to original image size\n",
    "classified_image = classified_pixels.reshape(image.shape)\n",
    "\n",
    "# Save the classified image\n",
    "cv2.imwrite(\"test_colour.png\", cv2.cvtColor(classified_image, cv2.COLOR_RGB2BGR))\n",
    "print(\"‚úÖ Classification completed. Saved as classified_image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "from scipy import signal\n",
    "\n",
    "SAMPLE_RATE = 44100\n",
    "\n",
    "def envelope(wave, attack=0.01, decay=0.2, sustain=0.8, release=0.1):\n",
    "    n = len(wave)\n",
    "    a = int(attack * SAMPLE_RATE)\n",
    "    d = int(decay * SAMPLE_RATE)\n",
    "    r = int(release * SAMPLE_RATE)\n",
    "    s = n - (a + d + r)\n",
    "    env = np.concatenate([\n",
    "        np.linspace(0, 1, a),                              # attack\n",
    "        np.linspace(1, sustain, d),                        # decay\n",
    "        np.ones(s) * sustain,                              # sustain\n",
    "        np.linspace(sustain, 0, r)                         # release\n",
    "    ])\n",
    "    return wave[:len(env)] * env  # Apply envelope\n",
    "\n",
    "def generate_sound(instrument, frequency=440, duration=1.0, amplitude=0.5):\n",
    "    t = np.linspace(0, duration, int(SAMPLE_RATE * duration), endpoint=False)\n",
    "\n",
    "    if instrument == 'flute':\n",
    "        repeat_interval = 0.5 # total cycle: silence + note\n",
    "        note_duration = repeat_interval - 0.1\n",
    "        samples_per_note = int(SAMPLE_RATE * note_duration)\n",
    "        silence = np.zeros(int(SAMPLE_RATE * 0.1))\n",
    "\n",
    "        # Generate one flute note chunk\n",
    "        t_chunk = np.arange(samples_per_note) / SAMPLE_RATE\n",
    "        note_chunk = np.sin(2 * np.pi * frequency * t_chunk)\n",
    "        note_chunk = envelope(note_chunk, decay=0.2, sustain=0.6)\n",
    "\n",
    "        # Combine silence + note\n",
    "        wave_chunk = np.concatenate([silence, note_chunk])\n",
    "\n",
    "        # Repeat the full chunk\n",
    "        num_repeats = int(np.ceil(duration / repeat_interval))\n",
    "        wave = np.tile(wave_chunk, num_repeats)\n",
    "\n",
    "        # Trim to exact duration\n",
    "        wave = wave[:int(SAMPLE_RATE * duration)]\n",
    "\n",
    "    elif instrument == 'piano':\n",
    "       \n",
    "        repeat_interval = 0.5  # seconds\n",
    "        samples_per_repeat = int(SAMPLE_RATE * repeat_interval)\n",
    "\n",
    "        # Generate one fixed note chunk\n",
    "        t_chunk = np.arange(samples_per_repeat) / SAMPLE_RATE\n",
    "        wave_chunk = (np.sin(2 * np.pi * frequency * t_chunk) +\n",
    "                    0.5 * np.sin(4 * np.pi * frequency * t_chunk))\n",
    "        wave_chunk = envelope(wave_chunk, decay=0.15, sustain=0.3)\n",
    "\n",
    "        # Repeat the exact same waveform\n",
    "        num_repeats = int(np.ceil(duration / repeat_interval))\n",
    "        wave = np.tile(wave_chunk, num_repeats)\n",
    "\n",
    "        # Trim to match the exact duration\n",
    "        wave = wave[:int(SAMPLE_RATE * duration)]\n",
    "\n",
    "    elif instrument == 'sax':\n",
    "        repeat_interval = 0.5  # seconds (includes 0.3s silence)\n",
    "        note_duration = repeat_interval - 0.1 # actual note length\n",
    "        samples_per_note = int(SAMPLE_RATE * note_duration)\n",
    "        silence = np.zeros(int(SAMPLE_RATE * 0.1))\n",
    "\n",
    "        # Create one sax note chunk\n",
    "        t_chunk = np.arange(samples_per_note) / SAMPLE_RATE\n",
    "        modulator = np.sin(2 * np.pi * 2 * frequency * t_chunk)\n",
    "        note_chunk = np.sin(2 * np.pi * frequency * t_chunk + 5 * modulator)\n",
    "        note_chunk = envelope(note_chunk, decay=0.25, sustain=0.6)\n",
    "\n",
    "        # Combine silence + note\n",
    "        wave_chunk = np.concatenate([silence, note_chunk])\n",
    "\n",
    "        # Repeat the full chunk\n",
    "        num_repeats = int(np.ceil(duration / repeat_interval))\n",
    "        wave = np.tile(wave_chunk, num_repeats)\n",
    "\n",
    "        # Trim to exact duration\n",
    "        wave = wave[:int(SAMPLE_RATE * duration)]\n",
    "\n",
    "\n",
    "    elif instrument == 'guitar':\n",
    "        N = int(SAMPLE_RATE / frequency)\n",
    "        output = []\n",
    "        decay_factor = 1.0\n",
    "        refresh_interval = 0.5 # Refresh every 5 seconds\n",
    "\n",
    "        for i in range(int(SAMPLE_RATE * duration)):\n",
    "            # Reinitialize the buffer every 'refresh_interval' seconds\n",
    "            if i % (SAMPLE_RATE * refresh_interval) == 0:\n",
    "                buffer = np.random.uniform(-1, 1, N)\n",
    "\n",
    "            avg = 0.5 * (buffer[0] + buffer[1])\n",
    "            output.append(avg)\n",
    "            buffer = np.append(buffer[1:], [avg * decay_factor])\n",
    "\n",
    "        wave = np.array(output)\n",
    "        wave = envelope(wave, attack=0.01, decay=0.05, sustain=1.0, release=0.1)\n",
    "\n",
    "    elif instrument == 'conga':\n",
    "        wave = np.zeros_like(t)\n",
    "        hit_interval = 0.5 # seconds\n",
    "        hit_samples = int(hit_interval * SAMPLE_RATE)\n",
    "\n",
    "        # Make a single conga hit\n",
    "        def single_conga_hit(length):\n",
    "            hit_t = np.linspace(0, length / SAMPLE_RATE, length, endpoint=False)\n",
    "            sine = np.sin(2 * np.pi * frequency * hit_t) * np.exp(-6 * hit_t)\n",
    "\n",
    "            # Slap noise\n",
    "            noise = np.random.normal(0, 1, length)\n",
    "            b, a = signal.butter(3, 800 / (0.5 * SAMPLE_RATE), btype='high')\n",
    "            slap = signal.lfilter(b, a, noise) * np.exp(-10 * hit_t)\n",
    "\n",
    "            hit = 0.6 * sine + 0.4 * slap\n",
    "            return envelope(hit, attack=0.001, decay=0.2, sustain=4.0, release=0.1)\n",
    "\n",
    "        hit_wave = single_conga_hit(int(0.5 * SAMPLE_RATE))  # 0.5s hit\n",
    "\n",
    "        for i in range(0, len(t), hit_samples):\n",
    "            end = min(i + len(hit_wave), len(wave))\n",
    "            wave[i:end] += hit_wave[:end - i]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown instrument\")\n",
    "\n",
    "    wave = amplitude * wave / np.max(np.abs(wave))\n",
    "    return wave\n",
    "\n",
    "def play_instrument(instrument, pitch=440, duration=1.0, amplitude=0.5):\n",
    "    wave = generate_sound(instrument, pitch, duration, amplitude)\n",
    "    return wave\n",
    "    # # print(f\"Playing {instrument} at {pitch} Hz for with amplitude {amplitude}\")\n",
    "    sd.play(wave, samplerate=SAMPLE_RATE)\n",
    "    sd.wait()\n",
    "\n",
    "\n",
    "# play_instrument('flute', pitch=440, duration=3.0, amplitude=1.0)\n",
    "# play_instrument('piano', pitch=440, duration=3.0, amplitude=1.0)\n",
    "# play_instrument('guitar', pitch=440, duration=3.0, amplitude=1.0)\n",
    "# play_instrument('sax', pitch=440, duration=3.0, amplitude=0.7)\n",
    "# play_instrument('conga', pitch=440, duration=3.0, amplitude=1.0)\n",
    "\n",
    "#save these sounds as .wav files\n",
    "from scipy.io import wavfile\n",
    "\n",
    "def save_sound(wave, filename=\"output.wav\"):\n",
    "    # Normalize the wave to be in the range of int16 (16-bit WAV)\n",
    "    wave_int16 = np.int16(wave * 32767)  # Convert to 16-bit PCM format\n",
    "    wavfile.write(filename, SAMPLE_RATE, wave_int16)\n",
    "    print(f\"Sound saved as {filename}\")\n",
    "\n",
    "save_sound(play_instrument('guitar', pitch=440, duration=5.0, amplitude=1.0), \"sounds1/red_guitar.wav\")\n",
    "save_sound(play_instrument('piano', pitch=440, duration=5.0, amplitude=1.0), \"sounds1/yellow_piano.wav\")\n",
    "save_sound(play_instrument('flute', pitch=440, duration=5.0, amplitude=1.0), \"sounds1/blue_flute.wav\")\n",
    "save_sound(play_instrument('conga', pitch=440, duration=5.0, amplitude=1.0), \"sounds1/green_conga.wav\")\n",
    "save_sound(play_instrument('sax', pitch=440, duration=5.0, amplitude=0.6), \"sounds1/black_sax.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745256440.899578   10779 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1745256440.900390   10919 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.1.3-1pop0~1689084530~22.04~0618746), renderer: Mesa Intel(R) Xe Graphics (TGL GT2)\n",
      "W0000 00:00:1745256440.919092   10910 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745256440.934450   10915 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Black\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Red\n",
      "Detected color: Black\n",
      "Detected color: Red\n",
      "Detected color: Black\n",
      "Detected color: Black\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pickle\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "\n",
    "# Load the KNN model\n",
    "with open(\"color_classifier.pkl\", \"rb\") as f:\n",
    "    knn = pickle.load(f)\n",
    "\n",
    "# Load classified image\n",
    "classified_image = cv2.imread(\"test_colour.png\")\n",
    "if classified_image is None:\n",
    "    print(\"‚ùå Error: Could not load classified image.\")\n",
    "    exit()\n",
    "\n",
    "# Load translucent overlay\n",
    "overlay = cv2.imread(\"test_og.png\", cv2.IMREAD_UNCHANGED)\n",
    "if overlay is None:\n",
    "    print(\"‚ùå Error: Could not load overlay.\")\n",
    "    overlay = np.zeros_like(classified_image)\n",
    "\n",
    "# Start camera\n",
    "# cap = cv2.VideoCapture(0)\n",
    "cap = cv2.VideoCapture('/dev/video2')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"‚ùå Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Get camera size\n",
    "cam_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "cam_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Resize overlay and classified image while keeping aspect ratio\n",
    "def resize_with_aspect_ratio(image, target_width, target_height):\n",
    "    \"\"\" Resize an image while maintaining its aspect ratio and adding padding. \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    aspect_ratio = w / h\n",
    "\n",
    "    if w > h:\n",
    "        new_w = target_width\n",
    "        new_h = int(target_width / aspect_ratio)\n",
    "    else:\n",
    "        new_h = target_height\n",
    "        new_w = int(target_height * aspect_ratio)\n",
    "\n",
    "    resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Add padding to match the target size\n",
    "    top = (target_height - new_h) // 2\n",
    "    bottom = target_height - new_h - top\n",
    "    left = (target_width - new_w) // 2\n",
    "    right = target_width - new_w - left\n",
    "\n",
    "    color = [0, 0, 0, 0] if image.shape[2] == 4 else [0, 0, 0]\n",
    "    return cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "\n",
    "classified_image = resize_with_aspect_ratio(classified_image, cam_width, cam_height)\n",
    "overlay = resize_with_aspect_ratio(overlay, cam_width, cam_height)\n",
    "\n",
    "# Ensure overlay has alpha channel\n",
    "if overlay.shape[2] == 3:\n",
    "    alpha_channel = np.ones((cam_height, cam_width), dtype=np.uint8) * 255\n",
    "    overlay = np.dstack((overlay, alpha_channel))\n",
    "\n",
    "alpha = overlay[:, :, 3] / 255.0\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "color_sounds = {\n",
    "    \"Red\": \"/home/singhasaur/code/HCAI/Project_w_sound/FINAL/sounds1/red_guitar.wav\",\n",
    "    \"Blue\": \"/home/singhasaur/code/HCAI/Project_w_sound/FINAL/sounds1/blue_flute.wav\",\n",
    "    \"Green\": \"/home/singhasaur/code/HCAI/Project_w_sound/FINAL/sounds1/green_conga.wav\",\n",
    "    \"Yellow\": \"/home/singhasaur/code/HCAI/Project_w_sound/FINAL/sounds1/yellow_piano.wav\",\n",
    "    \"Black\": \"/home/singhasaur/code/HCAI/Project_w_sound/FINAL/sounds1/black_sax.wav\",  \n",
    "}\n",
    "\n",
    "# Function to get color classification from classified image\n",
    "def get_classified_color(x, y):\n",
    "    bgr = classified_image[y, x]\n",
    "    rgb = bgr[::-1]  # Convert BGR to RGB\n",
    "    return knn.predict([rgb])[0]\n",
    "\n",
    "# Function to play sound (only when color changes)\n",
    "import threading\n",
    "\n",
    "last_color = None  # Track last detected color\n",
    "\n",
    "def play_sound(color):\n",
    "    global last_color\n",
    "\n",
    "    # üõë Stop all sounds if White is detected\n",
    "    if color == \"White\":\n",
    "        print(\"üîá White detected! Stopping all sounds.\")\n",
    "        sd.stop()\n",
    "        last_color = \"White\"\n",
    "        return  # Exit early\n",
    "\n",
    "    # üîç Check if the sound file exists\n",
    "    if color in color_sounds:\n",
    "        sound_file = color_sounds[color]\n",
    "    else:\n",
    "        print(f\"‚ùå No sound file mapped for {color}\")\n",
    "        return\n",
    "\n",
    "    # üõë Skip if same color as before\n",
    "    if color == last_color:\n",
    "        return\n",
    "\n",
    "    last_color = color  # Update last color\n",
    "\n",
    "    try:\n",
    "        data, samplerate = sf.read(sound_file)\n",
    "        sd.play(data, samplerate, blocking=False)  # ‚úÖ Non-blocking playback\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Sound error: {e}\")\n",
    "\n",
    " \n",
    "\n",
    "# Main loop\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    display_image = classified_image.copy()\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Get index finger tip\n",
    "            index_finger_tip = hand_landmarks.landmark[8]\n",
    "            x = int(index_finger_tip.x * cam_width)\n",
    "            y = int(index_finger_tip.y * cam_height)\n",
    "            x, y = np.clip(x, 0, cam_width - 1), np.clip(y, 0, cam_height - 1)\n",
    "            \n",
    "            # Get color classification\n",
    "            color_name = get_classified_color(x, y)\n",
    "            print(f\"Detected color: {color_name}\")\n",
    "\n",
    "            # Play sound only when color changes\n",
    "            play_sound(color_name)\n",
    "\n",
    "            # Show pointer on classified image\n",
    "            cv2.circle(display_image, (x, y), 8, (0, 255, 0), -1)\n",
    "            cv2.circle(frame, (x, y), 8, (255, 255, 255), -1)\n",
    "\n",
    "    # Apply overlay to camera frame\n",
    "    for c in range(3):\n",
    "        frame[:, :, c] = (1 - 0.4 * alpha) * frame[:, :, c] + (0.4 * alpha) * overlay[:, :, c]\n",
    "\n",
    "    cv2.imshow(\"Camera Feed with Overlay\", frame)\n",
    "    cv2.imshow(\"Classified Image\", display_image)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        sd.stop() \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
